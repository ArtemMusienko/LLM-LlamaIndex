![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)![HuggingFace](https://img.shields.io/badge/huggingface-%23FFD21E.svg?style=for-the-badge&logo=huggingface&logoColor=white)

## LLM LlamaIndex

В коде реализован подход с фиксированием версий ряда библиотек, с которым работает готовый продукт. Это сделано для удобства и стабильности кода.

>Для использования кода используйте
> сгенерированный токен на **[HuggingFace](https://huggingface.co/settings/profile)** в настройках вашего профиля.

Для общего понимания давайте разберем важные термины в этой работе:

---
**Llama Index** — это фреймворк для создания приложений, работающих с собственными данными, на основе больших языковых моделей. Он предоставляет инструменты для индексации, обработки и интеграции разнообразных источников данных в структурированный формат, понятный для **LLM**. Основная его задача — эффективное извлечение релевантной информации и её контекстуализация для улучшения ответов модели. Таким образом, **Llama Index** служит мощным связующим звеном между приватными данными и возможностями языковых моделей.

**Gradio** — это библиотека с открытым исходным кодом для быстрого создания удобных веб-интерфейсов для машинного обучения и других программ на Python. Она позволяет разрабатывать интерактивные демонстрации моделей с помощью всего нескольких строк кода, предлагая готовые компоненты вроде полей ввода, слайдеров и графиков. Благодаря простоте использования **Gradio** ускоряет процесс тестирования, совместного использования и развертывания моделей. Это делает модели машинного обучения более доступными для пользователей без технического бэкграунда.

---

Здесь используется модель *[IlyaGusev/saiga_mistral_7b](https://huggingface.co/IlyaGusev/saiga_mistral_7b_gguf)*. В качестве эмбеддинга будем использовать *[sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)*, которая хорошо работает с разными языками. 
В качестве базы знаний, которая подается в *RAG-систему* будем использовать данную [статью](https://ru.wikipedia.org/wiki/TensorFlow). 

Также в коде представлена простая реализация **Gradio-интерфейса** с базовым функционалом: *распарсить данные* и *отправить запрос*. 

Важно!

> Этот код получится выполнить только с использованием аппаратного
> ускорителя. Я использовал графический **ускоритель T4** в **Google Colab**.
